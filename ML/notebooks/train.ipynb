{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c4f2404",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95bea42a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import Tuple, Optional, Iterable, Callable, List, Dict\n",
    "from collections import OrderedDict\n",
    "from copy import deepcopy\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.models import (\n",
    "    swin_s,\n",
    "    Swin_S_Weights,\n",
    "    efficientnet_b7,\n",
    "    EfficientNet_B7_Weights,\n",
    "    efficientnet_b3,\n",
    "    EfficientNet_B3_Weights,\n",
    "    swin_v2_t,\n",
    "    Swin_V2_T_Weights,\n",
    "    swin_v2_s,\n",
    "    Swin_V2_S_Weights,\n",
    "    vit_b_16,\n",
    "    ViT_B_16_Weights,\n",
    "    densenet201,\n",
    "    DenseNet201_Weights,\n",
    "    googlenet,\n",
    "    GoogLeNet_Weights,\n",
    "    convnext_small,\n",
    "    ConvNeXt_Small_Weights,\n",
    "    regnet_y_8gf,\n",
    "    RegNet_Y_8GF_Weights,\n",
    ")\n",
    "from torchvision.models.swin_transformer import Permute\n",
    "from torchvision.io import read_image\n",
    "from torchvision import transforms\n",
    "from torchvision.ops import FeaturePyramidNetwork\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "067d9c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root = Path(\"data/dataset_crop\")\n",
    "\n",
    "df = pd.read_csv(data_root / \"thread_depths.csv\")\n",
    "df_train, df_val = train_test_split(df, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a046f200",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ThreadDataset(Dataset):\n",
    "    def __init__(self, data: pd.DataFrame, data_root_dir: str, transform: Optional[nn.Module] = None):\n",
    "        self.data = data\n",
    "        self.data_root_dir = Path(data_root_dir)\n",
    "        self.image_paths = []\n",
    "        self.labels = []\n",
    "        for _, row in self.data.iterrows():\n",
    "            image_path = self.data_root_dir / row[\"path\"]\n",
    "            if not image_path.exists():\n",
    "                print(f\"Warning: {image_path} does not exist\")\n",
    "                continue\n",
    "            self.image_paths.append(self.data_root_dir / row[\"path\"])\n",
    "            self.labels.append(row[\"label\"])\n",
    "\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, float]:\n",
    "        image_path = self.image_paths[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        image = read_image(str(image_path))\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3caa9dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = data_root / df.sample().iloc[0, 0]\n",
    "img = read_image(image_path)\n",
    "\n",
    "\n",
    "class Clahe(nn.Module):\n",
    "    def __init__(self, clip_limit: float = 2.0):\n",
    "        super().__init__()\n",
    "        self.clip_limit = clip_limit\n",
    "        self.clahe = cv2.createCLAHE(clipLimit=clip_limit)\n",
    "\n",
    "    def _apply_clahe(self, img: np.ndarray) -> np.ndarray:\n",
    "        result = []\n",
    "        for channel in img:\n",
    "            result.append(self.clahe.apply(channel))\n",
    "\n",
    "        return np.stack(result, axis=0)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        device = x.device\n",
    "        x_npy = x.cpu().numpy()\n",
    "        x_clahe = self._apply_clahe(x_npy)\n",
    "\n",
    "        return torch.tensor(x_clahe, device=device)\n",
    "\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "        # Clahe(),\n",
    "        lambda x: x / 255,\n",
    "        transforms.Resize(\n",
    "            (512, 512), interpolation=transforms.InterpolationMode.BICUBIC\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "transform_aug = transforms.Compose(\n",
    "    [\n",
    "        transform,\n",
    "        # transforms.ToPILImage(),\n",
    "        # transforms.RandAugment(\n",
    "        #     num_ops=3, interpolation=transforms.InterpolationMode.BICUBIC\n",
    "        # ),\n",
    "        # transforms.ToTensor(),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomVerticalFlip(),\n",
    "        # transforms.RandomAffine(15, (0.05, 0.05), fill=255),\n",
    "    ]\n",
    ")\n",
    "\n",
    "img = transform_aug(img)\n",
    "\n",
    "plt.imshow(img.permute(1, 2, 0))\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d4ea233",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ThreadDataset(df_train, data_root, transform_aug)\n",
    "val_dataset = ThreadDataset(df_val, data_root, transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, shuffle=True, num_workers=8, batch_size=8)\n",
    "val_loader = DataLoader(val_dataset, shuffle=False, num_workers=8, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c60fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {}\n",
    "\n",
    "# model = resnet50(weights=ResNet50_Weights.IMAGENET1K_V2)\n",
    "# model.fc = nn.Sequential(nn.Linear(2048, 512), nn.ReLU(), nn.Linear(512, 1))\n",
    "\n",
    "models[\"swin_v2\"] = swin_v2_s(weights=Swin_V2_S_Weights.IMAGENET1K_V1)\n",
    "models[\"swin_v2\"].head = nn.Sequential(nn.Linear(768, 256), nn.ReLU(), nn.Linear(256, 1))\n",
    "\n",
    "model = swin_s(weights=Swin_S_Weights.IMAGENET1K_V1)\n",
    "model.head = nn.Sequential(nn.Linear(768, 256), nn.ReLU(), nn.Linear(256, 1))\n",
    "\n",
    "models[\"effnet_b3\"] = efficientnet_b3(weights=EfficientNet_B3_Weights.IMAGENET1K_V1)\n",
    "models[\"effnet_b3\"].classifier = nn.Sequential(nn.Linear(1536, 512), nn.SiLU(), nn.Linear(512, 1))\n",
    "\n",
    "models[\"effnet_b7\"] = efficientnet_b7(weights=EfficientNet_B7_Weights.IMAGENET1K_V1)\n",
    "models[\"effnet_b7\"].classifier = nn.Sequential(nn.Linear(2560, 512), nn.SiLU(), nn.Linear(512, 1))\n",
    "\n",
    "model = swin_v2_t(weights=Swin_V2_T_Weights.IMAGENET1K_V1)\n",
    "model.features[0][0] = nn.Conv2d(in_channels=3, out_channels=96, kernel_size=(5, 5), stride=(2, 2))\n",
    "model.head = nn.Sequential(nn.Linear(768, 512), nn.GELU(), nn.Linear(512, 1))\n",
    "\n",
    "# model = vit_b_16(weights=ViT_B_16_Weights.IMAGENET1K_SWAG_LINEAR_V1)\n",
    "# model.heads = nn.Sequential(nn.Linear(768, 512), nn.GELU(), nn.Linear(512, 1))\n",
    "\n",
    "models[\"densenet201\"] = densenet201(weights=DenseNet201_Weights.IMAGENET1K_V1)\n",
    "models[\"densenet201\"].classifier = nn.Sequential(nn.Linear(1920, 512), nn.ReLU(), nn.Linear(512, 1))\n",
    "\n",
    "models[\"googlenet\"] = googlenet(weights=GoogLeNet_Weights.IMAGENET1K_V1)\n",
    "models[\"googlenet\"].fc = nn.Sequential(nn.Linear(1024, 512), nn.ReLU(), nn.Linear(512, 1))\n",
    "\n",
    "# model = convnext_small(weights=ConvNeXt_Small_Weights.IMAGENET1K_V1)\n",
    "# model.classifier[-1] = nn.Sequential(nn.Linear(768, 512), nn.GELU(), nn.Linear(512, 1))\n",
    "\n",
    "# model = regnet_y_8gf(weights=RegNet_Y_8GF_Weights.IMAGENET1K_V2)\n",
    "# model.fc = nn.Sequential(nn.Linear(2016, 512), nn.ReLU(), nn.Linear(512, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "597c27ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "efficientnet_b3(weights=EfficientNet_B3_Weights.IMAGENET1K_V1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e766da6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fn(\n",
    "    model: nn.Module,\n",
    "    train_loader: DataLoader,\n",
    "    val_loader: DataLoader,\n",
    "    num_epochs: int = 10,\n",
    "):\n",
    "    best_val_metric = -torch.inf\n",
    "    best_model = deepcopy(model).cpu()\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer,\n",
    "        mode=\"max\",\n",
    "        factor=0.1,\n",
    "        patience=3,\n",
    "    )\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        running_loss = 0.0\n",
    "        for images, labels in tqdm(\n",
    "            train_loader,\n",
    "            desc=f\"Epoch {epoch + 1} Training\",\n",
    "            total=len(train_loader),\n",
    "        ):\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device, torch.float32).unsqueeze(1)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(images).exp()\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        print(\n",
    "            f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {running_loss / len(train_loader):.4f}\"\n",
    "        )\n",
    "\n",
    "        model.eval()\n",
    "\n",
    "        residuals = []\n",
    "        with torch.no_grad():\n",
    "            for images, labels in tqdm(\n",
    "                val_loader, desc=f\"Epoch {epoch + 1} Evalutaion\"\n",
    "            ):\n",
    "                images = images.to(device)\n",
    "                labels = labels.to(device).unsqueeze(1)\n",
    "\n",
    "                outputs = model(images).exp()\n",
    "                residuals.extend(torch.abs(outputs - labels).detach().cpu().tolist())\n",
    "        residuals = torch.tensor(residuals)\n",
    "        val_metric = torch.mean(torch.where(residuals <= 1, 1.0, 0.0))\n",
    "        print(\n",
    "            f\"Validation MAE: {torch.mean(residuals):.4f}, \"\n",
    "            + f\"Fraction of errors <= 1: {val_metric:.4f}, \"\n",
    "            + f\"0.9th quantile: {torch.quantile(residuals, 0.9):.4f}\"\n",
    "        )\n",
    "        scheduler.step(val_metric)\n",
    "\n",
    "        if val_metric > best_val_metric:\n",
    "            best_val_metric = val_metric\n",
    "            best_model = deepcopy(model).cpu()\n",
    "\n",
    "    model.cpu()\n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec3554c",
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_models: Dict[str, nn.Module] = {}\n",
    "for model_name, model in models.items():\n",
    "    print(f\"TRAINING {model_name}\")\n",
    "    model = train_fn(\n",
    "        model,\n",
    "        train_loader,\n",
    "        val_loader,\n",
    "        num_epochs=50,\n",
    "    )\n",
    "    print()\n",
    "    trained_models[model_name] = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7aa3e96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name, model in trained_models.items():\n",
    "    torch.save(model.state_dict(), f\"checkpoints/{model_name}.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "artif",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
